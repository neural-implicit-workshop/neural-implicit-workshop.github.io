---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: default
---
### Overview

Computer vision research has recently seen impressive success in creating implicit neural representations of scenes, such as neural radiance fields (NeRFs) and deep Signed Distance Functions (DeepSDFs). Similarly, implicit representations (not necessarily learned) such as SDFs and Riemannian Motion Policies (RMPs) have been used with great success in robotic motion planning, manipulation, and perception. In this workshop, we seek to explore the future of implicit neural representations in robotics. 

Through invited talks and a poster session, we will discuss questions such as:
- how to quickly learn implicit neural representations online from sensor data, 
- how to tractably reason about uncertainty in these representations, 
- how to leverage learned implicit representations for robotic localization, motion planning, manipulation, and locomotion tasks, 
- how to embed semantic understanding in such representations, and 
- how to adapt them to simulate and predict dynamic and uncertain scenes. 

We hope that this workshop will fuel interest and promote research in learning implicit scene representations for applications in robotics.

---
### Speakers

{% include speakers.html %}

---

### Organizers

- [Shreyas Kousik](https://www.shreyaskousik.com/){:.link-dark}, Stanford
- [Preston Culbertson](https://pculbertson.github.io/){:.link-dark}, Stanford
- [Jeannette Bohg](https://web.stanford.edu/~bohg/){:.link-dark}, Stanford
- [Mac Schwager](https://web.stanford.edu/~schwager/){:.link-dark}, Stanford

